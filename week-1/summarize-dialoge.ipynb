{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI Use Case: Summarize Dialogue\n",
    "\n",
    "We will do the dialogue summarization task using generative AI. We will explore how the input text affects the output of the model, and perform prompt engineering to direct it towards the task we need. \n",
    "\n",
    "By comparing zero shot, one shot, and few shot inferences, we will take the first step towards prompt engineering and see how it can enhance the generative output of Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Set up Kernel and Required Dependencies\n",
    "\n",
    "```\n",
    "pip3 install --upgrade pip\n",
    "\n",
    "pip3 install --disable-pip-version-check \\\n",
    "    torch==2.0.0 \\\n",
    "    torchdata==0.6.0\n",
    "\n",
    "pip3 install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets, Large Language Model (LLM), tokenizer, and configurator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Import Dataset and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this use case, we will be generating a summary of a dialogue with the pre-trained Large Language Model (LLM) [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) from Hugging Face. The list of available models in the Hugging Face `transformers` package can be found [here](https://huggingface.co/docs/transformers/index).\n",
    "\n",
    "Let's upload some simple dialogues from the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. This dataset contains 10,000+ dialogues with the corresponding manually labeled summaries and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "from enum import Enum\n",
    "class Dataset_Splits(Enum):\n",
    "    TRAIN = 'train'\n",
    "    VALIDATION = 'validation'\n",
    "    TEST = 'test'\n",
    "\n",
    "\n",
    "class Dataset_Columns(Enum):\n",
    "    DIALOGUE = 'dialogue'\n",
    "    SUMMARY = 'summary'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a few dialogues from the dataset with their baseline summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# only 2 rows in the dataset\n",
    "example_indices = [40, 200]\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print(\"Example \", i+1)\n",
    "    print(dash_line)\n",
    "\n",
    "    print(\"INPUT DIALOGUE:\")\n",
    "    print(dataset[Dataset_Splits.TEST.value][index][Dataset_Columns.DIALOGUE.value])\n",
    "\n",
    "    print(dash_line)\n",
    "    print(\"BASELINE HUMAN SUMMARY:\")\n",
    "    print(dataset[Dataset_Splits.TEST.value][index][Dataset_Columns.SUMMARY.value])\n",
    "    print(dash_line)\n",
    "\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the FLAN-T5 model, creating an instance of the `AutoModelForSeq2SeqLM` class with the `.from_pretrained()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Setup the Tokenizer\n",
    "\n",
    "To perform encoding and decoding, we need to work with text in tokenized form.\n",
    "\n",
    "Download the tokenizer for the FLAN-T5 model using `AutoTokenizer.from_pretrained()` method. Parameter `use_fast` switches on fast tokenizer. \n",
    "Find the tokenizer parameters in the [documentation](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 3.88MB/s]\n",
      "(…)flan-t5-base/resolve/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 11.4MB/s]\n",
      "(…)ase/resolve/main/special_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 9.96MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the tokenizer encoding and decoding a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE:\n",
      "[363, 97, 19, 34, 6, 3059, 58, 1]\n",
      "\n",
      "DECODED SENTENCE:\n",
      "What time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"What time is it, Tom?\"\n",
    "\n",
    "sentence_encoded = tokenizer.encode(sentence)\n",
    "sentence_decoded = tokenizer.decode(\n",
    "    sentence_encoded,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"ENCODED SENTENCE:\")\n",
    "print(sentence_encoded)\n",
    "print(\"\\nDECODED SENTENCE:\")\n",
    "print(sentence_decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Summarize Dialogues without Prompt Engineering\n",
    "\n",
    "Now it's time to explore how well the base LLM summarizes a dialogue without **any** prompt engineering. Prompt engineering is an act of a human changing the prompt (input) to improve the response for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT ANY PROMPT ENGINEERING:\n",
      "Person1: It's ten to nine.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT ANY PROMPT ENGINEERING:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset[Dataset_Splits.TEST.value][index][Dataset_Columns.DIALOGUE.value]\n",
    "    summary = dataset[Dataset_Splits.TEST.value][index][Dataset_Columns.SUMMARY.value]\n",
    "\n",
    "    # return_tensors – (optional) can be set to 'tf' or 'pt' to \n",
    "    # return respectively TensorFlow tf. constant or PyTorch torch.Tensor\n",
    "    # instead of a list of python integers.\n",
    "    # Need to pass in list of Pytorch.Tensors to generate() instead of \n",
    "    # a list of Python integers\n",
    "    inputs = tokenizer.encode(dialogue, return_tensors='pt')\n",
    "    model_completion = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens = 50\n",
    "    )\n",
    "    output = tokenizer.decode(model_completion[0], skip_special_tokens=True)\n",
    "\n",
    "    print(dash_line)\n",
    "    print(\"Example \", i+1)\n",
    "    print(dash_line)\n",
    "    print(f\"INPUT PROMPT:\\n{dialogue}\")\n",
    "    print(dash_line)\n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\")\n",
    "    print(dash_line)\n",
    "    print(f\"MODEL GENERATION - WITHOUT ANY PROMPT ENGINEERING:\\n{output}\\n\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the guesses of the model make some sense, but it doesn't seem to be sure what task it is supposed to accomplish. Prompt engineering can help here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Summarize Dialogue with an Instruction Prompt\n",
    "\n",
    "Prompt engineering is an important concept in using foundation models for text generation. [This blog](https://www.amazon.science/blog/emnlp-prompt-engineering-is-the-new-feature-engineering) from Amazon Science is a quick introduction to prompt engineering.\n",
    "\n",
    "### 5.1 - Zero Shot Inference with an Instruction Prompt\n",
    "\n",
    "In order to instruct the model to perform a task - summarize a dialogue - we can take the dialogue and convert it into an instruction prompt. \n",
    "This is often called zero shot inference. We can check out [this blog](https://aws.amazon.com/blogs/machine-learning/zero-shot-prompting-for-the-flan-t5-foundation-model-in-amazon-sagemaker-jumpstart/) from AWS for a quick description of what zero shot learning is and why it is an important concept to the LLM model.\n",
    "\n",
    "Wrap the dialogue in a descriptive instruction and see how the generated text will change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT WITH INSTRUCTION PROMPT:\n",
      "The train is about to leave.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT WITH INSTRUCTION PROMPT:\n",
      "#Person1#: You'd probably want to upgrade your computer. #Person2#: You could also upgrade your hardware. #Person1#: You'd probably want a faster processor, more memory and a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset[Dataset_Splits.TEST.value][index][Dataset_Columns.DIALOGUE.value]\n",
    "    summary = dataset[Dataset_Splits.TEST.value][index][Dataset_Columns.SUMMARY.value]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation:\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    model_completion = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens = 50\n",
    "    )\n",
    "    output = tokenizer.decode(model_completion[0], skip_special_tokens=True)\n",
    "\n",
    "    print(dash_line)\n",
    "    print(\"Example \", i+1)\n",
    "    print(dash_line)\n",
    "    print(f\"INPUT PROMPT:\\n{prompt}\")\n",
    "    print(dash_line)\n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\")\n",
    "    print(dash_line)\n",
    "    print(f\"MODEL GENERATION - ZERO SHOT WITH INSTRUCTION PROMPT:\\n{output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better! But the model still does not pick up on the nuance of the conversations though.\n",
    "\n",
    "#### Exercise:\n",
    "\n",
    "- Experiment with the `prompt` text and see how the inferences will be changed. Will the inferences change if you end the prompt with just empty string vs. `Summary:`?\n",
    "- Try to rephrase the beginning of the `prompt` text from `Summarize the following conversation`. to something different - and see how it will influence the generated output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT WITH INSTRUCTION PROMPT:\n",
      "The train is about to leave.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT WITH INSTRUCTION PROMPT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset[Dataset_Splits.TEST.value][index][Dataset_Columns.DIALOGUE.value]\n",
    "    summary = dataset[Dataset_Splits.TEST.value][index][Dataset_Columns.SUMMARY.value]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation:\n",
    "{dialogue}\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    model_completion = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens = 50\n",
    "    )\n",
    "    output = tokenizer.decode(model_completion[0], skip_special_tokens=True)\n",
    "\n",
    "    print(dash_line)\n",
    "    print(\"Example \", i+1)\n",
    "    print(dash_line)\n",
    "    print(f\"INPUT PROMPT:\\n{prompt}\")\n",
    "    print(dash_line)\n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\")\n",
    "    print(dash_line)\n",
    "    print(f\"MODEL GENERATION - ZERO SHOT WITH INSTRUCTION PROMPT:\\n{output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not having `Summary:` in the prompt didn't affect the 1st example, but the second example is worse than it was before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation to capture the essence of what is going on:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT WITH INSTRUCTION PROMPT:\n",
      "The train is about to leave.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation to capture the essence of what is going on:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT WITH INSTRUCTION PROMPT:\n",
      "#Person1#: You might want to upgrade your computer. #Person2#: You could also add a painting program to your software. #Person1#: You could also add a CD-ROM drive.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset[Dataset_Splits.TEST.value][index][Dataset_Columns.DIALOGUE.value]\n",
    "    summary = dataset[Dataset_Splits.TEST.value][index][Dataset_Columns.SUMMARY.value]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation to capture the essence of what is going on:\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    model_completion = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens = 50\n",
    "    )\n",
    "    output = tokenizer.decode(model_completion[0], skip_special_tokens=True)\n",
    "\n",
    "    print(dash_line)\n",
    "    print(\"Example \", i+1)\n",
    "    print(dash_line)\n",
    "    print(f\"INPUT PROMPT:\\n{prompt}\")\n",
    "    print(dash_line)\n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\")\n",
    "    print(dash_line)\n",
    "    print(f\"MODEL GENERATION - ZERO SHOT WITH INSTRUCTION PROMPT:\\n{output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Tell me the topic of the conversation. Pick up to 3 topics:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "Topics(s):\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT WITH INSTRUCTION PROMPT:\n",
      "Tom: What time is it?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Tell me the topic of the conversation. Pick up to 3 topics:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Topics(s):\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT WITH INSTRUCTION PROMPT:\n",
      "Computer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset[Dataset_Splits.TEST.value][index][Dataset_Columns.DIALOGUE.value]\n",
    "    summary = dataset[Dataset_Splits.TEST.value][index][Dataset_Columns.SUMMARY.value]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Tell me the topic of the conversation. Pick up to 3 topics:\n",
    "{dialogue}\n",
    "\n",
    "Topics(s):\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    model_completion = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens = 50\n",
    "    )\n",
    "    output = tokenizer.decode(model_completion[0], skip_special_tokens=True)\n",
    "\n",
    "    print(dash_line)\n",
    "    print(\"Example \", i+1)\n",
    "    print(dash_line)\n",
    "    print(f\"INPUT PROMPT:\\n{prompt}\")\n",
    "    print(dash_line)\n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\")\n",
    "    print(dash_line)\n",
    "    print(f\"MODEL GENERATION - ZERO SHOT WITH INSTRUCTION PROMPT:\\n{output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Zero Shot Inference with Prompt Templates from FLAN-T5\n",
    "\n",
    "FLAN-T5 has many prompt templates that are published for certain tasks [here](https://github.com/google-research/FLAN/tree/main/flan/v2). In the following code, you will use one of the [pre-built FLAN-T5 prompts](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT WITH PROMPT TEMPLATE:\n",
      "Tom is late for the train.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT WITH PROMPT TEMPLATE:\n",
      "#Person1#: You could add a painting program to your software. #Person2#: That would be a bonus. #Person1#: You might also want to upgrade your hardware. #Person1#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset[Dataset_Splits.TEST.value][index][Dataset_Columns.DIALOGUE.value]\n",
    "    summary = dataset[Dataset_Splits.TEST.value][index][Dataset_Columns.SUMMARY.value]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Dialogue:\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    model_completion = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens = 50\n",
    "    )\n",
    "    output = tokenizer.decode(model_completion[0], skip_special_tokens=True)\n",
    "\n",
    "    print(dash_line)\n",
    "    print(\"Example \", i+1)\n",
    "    print(dash_line)\n",
    "    print(f\"INPUT PROMPT:\\n{prompt}\")\n",
    "    print(dash_line)\n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\")\n",
    "    print(dash_line)\n",
    "    print(f\"MODEL GENERATION - ZERO SHOT WITH PROMPT TEMPLATE:\\n{output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prompt from FLAN-T5 did help a bit, but still struggles to pick up on the nuance of the conversation. This is what we will try to solve with the few shot inferencing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
